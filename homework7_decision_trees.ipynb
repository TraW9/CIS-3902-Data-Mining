{
 "cells": [
  {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/catawba-data-mining/CIS-3902-Data-Mining/blob/main/homework7_decision_trees.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
       ]
    },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees, Random Forests and the Confusion Matrix for Evaluation\n",
    "## Data Mining Catawba College\n",
    "## Group Members:\n",
    "## 1.\n",
    "## 2.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Directions: Open in COLAB.  Carefully study the documentation and execute the code.  Answer the questions throughout the lab and at the end of the lab. They are labeled \"Question 1\" etc. Save your work and turn in the .ipynb file.\n",
    "\n",
    "For this project we will be exploring publicly available data from [LendingClub.com](www.lendingclub.com). Lending Club connects people who need money (borrowers) with people who have money (investors). Hopefully, as an investor you would want to invest in people who showed a profile of having a high probability of paying you back. We will try to create a model that will help predict this.\n",
    "\n",
    "### Research Goal:  Develop a predictive model using decision trees to determine applicants that are most likely to be able to pay back their loans.\n",
    "\n",
    "Lending club had a [very interesting year in 2016](https://en.wikipedia.org/wiki/Lending_Club#2016), so let's check out some of their data and keep the context in mind. This data is from before they even went public.\n",
    "\n",
    "We will use lending data from 2007-2010 and be trying to classify and predict whether or not the borrower paid back their loan in full. You can download the original data from [here](https://www.lendingclub.com/info/download-data.action) but we are using a csv provided as it has been cleaned of NA values or NULL values.\n",
    "\n",
    "### Data Understanding - Here are what the columns represent:\n",
    "* int.rate: The interest rate of the loan, as a proportion (a rate of 11% would be stored as 0.11). Borrowers judged by LendingClub.com to be more risky are assigned higher interest rates.\n",
    "* installment: The monthly installments owed by the borrower if the loan is funded.\n",
    "* log.annual.inc: The natural log of the self-reported annual income of the borrower.\n",
    "* dti: The debt-to-income ratio of the borrower (amount of debt divided by annual income).\n",
    "* fico: The FICO credit score of the borrower.\n",
    "* days.with.cr.line: The number of days the borrower has had a credit line.\n",
    "* revol.bal: The borrower's revolving balance (amount unpaid at the end of the credit card billing cycle).\n",
    "* revol.util: The borrower's revolving line utilization rate (the amount of the credit line used relative to total credit available).\n",
    "* inq.last.6mths: The borrower's number of inquiries by creditors in the last 6 months.\n",
    "* pub.rec: The borrower's number of derogatory public records (bankruptcy filings, tax liens, or judgments).\n",
    "* fully.paid - yes or 1 if the loan was paid back in full (yes is good), no or 0  means it was not paid back in full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.  Import Libraries\n",
    "\n",
    "**Import the usual libraries for pandas and plotting. You can import sklearn later on.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Get the Data\n",
    "\n",
    "** Use pandas to read loan_data.csv as a dataframe called loans.  Remember this file has the NULL values removed. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/catawba-data-mining/CIS-3902-Data-Mining/main/loan_data_prep.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Look at the first few records, the attributes, and attributes types.\n",
    "** Check out the info(), head(), and describe() methods on loans.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  4.  Exploratory Data Analysis:  Visualization\n",
    "\n",
    "Let's do some data visualization!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.hist(df[df['credit.policy']==1]['fico'],color='blue',bins=30,alpha=0.5,edgecolor='blue')\n",
    "plt.hist(df[df['credit.policy']==0]['fico'],color='red',bins=30,alpha=0.5,edgecolor='red')\n",
    "plt.legend(['credit.policy=1','credit.policy=0'])\n",
    "plt.xlabel(\"FICO\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QUESTION 1:  (open up the markdown cell and key in your answer).  What does the above chart show about the relationship of FICO score and Credit Policy?<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Create a histogram -  select by the not.fully.paid column.**<br>\n",
    "** fully paid = 1 is good **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.hist(df[df['fully.paid']==1]['fico'],color='red',bins=30,alpha=0.5,edgecolor='red')\n",
    "plt.hist(df[df['fully.paid']==0]['fico'],color='blue',bins=30,alpha=0.5,edgecolor='blue')\n",
    "plt.legend(['fully.paid=1','fully.paid=0'])\n",
    "plt.xlabel(\"FICO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QUESTION 2: What does the above chart show about the relationship of FICO score and Not Fully Paid?<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Let's see the trend between FICO score and interest rate.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot('fico','int.rate',data=df,color='purple',size=8,ratio=3,space=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QUESTION 3: In the chart above, do you see a trend between FICO score and Interest Rate?<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Create the following lmplots to see if the trend differed between not.fully.paid and credit.policy.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot('fico','int.rate',data=df,hue='credit.policy',col='fully.paid',size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QUESTION 4: What do the above two charts show regarding the trend between FICO score and Interest Rate, and how it may or may not differ depending on the \"Not Fully Paid\" Attribute? <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.  Data Preparation\n",
    "\n",
    "Let's get our data ready for decision trees and random forest modeling.  <br><br>\n",
    "A decision tree is a flowchart-like tree structure where an internal node represents feature(or attribute), the branch represents a decision rule, and each leaf node represents the outcome. The topmost node in a decision tree is known as the root node. It learns to partition on the basis of the attribute value. It partitions the tree in recursively manner call recursive partitioning. This flowchart-like structure helps you in decision making. It's visualization like a flowchart diagram which easily mimics the human level thinking. That is why decision trees are easy to understand and interpret.<br><br>\n",
    "Random Forests are like a collection of decision trees taken from random samples from the data:\n",
    "It works in four steps:<br>   Select random samples from a given dataset.<br>\n",
    "    Construct a decision tree for each sample and get a prediction result from each decision tree.<br>\n",
    "    Perform a vote for each predicted result.<br>\n",
    "    Select the prediction result with the most votes as the final prediction.<br><br>\n",
    "Random Forests almost always outperform Decision Trees.\n",
    "\n",
    "\n",
    "** Let's start by checking loans.info() again.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We are using Python's scikit-learn to make our modeling process easier with this powerful library.<br><br>\n",
    "<p>It is important to understand how to prepare data for each type of modeling.</p>\n",
    "<p> We have a target variable, Not Fully Paid, to predict if we are going to get paid back or not. Since we have a target variable this is Supervised learning.</p><p>We have already removed nulls, this is generally important although just simply removing them isn't always the best practice because you might remove too much of the data!  More on this later.  </p>\n",
    "<p>Decision trees handle numerical values and categorical values, although scikit-learn requires us to change the categorical variables to \"dummy\" variables.  This means if we have a Cateorical Variable Direction with North, South, East and West as the possible categories, this will change to three variables (not four) North, South, and East.  If the direction for a particular record was \"North\", then North will have \"1\" and the rest will have \"0\". If the direction is \"West\" then the three will have 0 . . . all 0's mean the last category or West.</p>\n",
    "<p>The original data set has a categorical feature \"Loan Purpose\" which has been removed from the data due to its lack of importance in the prediction</p>\n",
    "<p> Numerical features for decision trees do not have to be \"normalized\" meaning if you have numerical variables in different ranges, you do not have to convert them to a similar scale.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test.  We train our model on the training data, and test the accuracy of the model we have built on test.\n",
    "\n",
    "Now its time to split our data into a training set and a testing set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our target is the fully paid variable\n",
    "# the train set will be trained on the target so we can learn what it means\n",
    "# again this is supervised learning\n",
    "# we will test our model on the test set\n",
    "X = df.drop('fully.paid',axis=1)\n",
    "y = df['fully.paid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train will have 70% of the data, test will have 30%\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.  Modeling:  Training a Decision Tree Model\n",
    "\n",
    "Let's start by training a single decision tree first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dTree = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dTree.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this may take a few minutes to run\n",
    "tree.plot_tree(dTree) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets take a look \n",
    "#this also takes a minute . . . or two\n",
    "text_representation = tree.export_text(dTree)\n",
    "print(text_representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another way to visualize the tree\n",
    "# this tree has lots of branches\n",
    "fn=['credit.policy','int.rate','installment','log.annual.inc','dti','fico','days.with.cr.line','revol.bal','revol.util','inq.last.6mths']\n",
    "cn=['0','1']\n",
    "fig, axes = plt.subplots(nrows = 1,ncols = 1,figsize = (14,14), dpi=300)\n",
    "tree.plot_tree(dTree,\n",
    "               feature_names = fn, \n",
    "               class_names=cn,\n",
    "               filled = True);fig.savefig('imagename.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Predictions and Evaluation of Decision Tree\n",
    "**Create predictions from the test set and create a confusion matrix and classfication report to evaluate our model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the predictions from the test set\n",
    "predictions = dTree.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary libraries from sklearn.metrics\n",
    "from sklearn.metrics import classification_report,confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation using a confusion matrix\n",
    "A confusion matrix is a great way to evaluate a classifcation model.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the confusion matrix\n",
    "cf_matrix = confusion_matrix(y_test,predictions)\n",
    "print(cf_matrix)\n",
    "# Can we look at this in a better, more intuitive way? What does this mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use seaborn to make the confusion matrix more intuitive\n",
    "# True Positives are Predicted Yes, Actual Yes or 2019\n",
    "# True Negatives are Predicted No, Actual No or 93\n",
    "# False Positives are Predicted Yes, Actual No or 350\n",
    "# False Negatives are Predicted No, Actual Yes or 412\n",
    "# Total = 2019+93+350+412 = 2874\n",
    "# Remember this is the test set - 30% of total records\n",
    "\n",
    "xlabels = ['Predicted: No','Predicted: Yes']\n",
    "ylabels = ['Actual: No', 'Actual: Yes']\n",
    "sns.heatmap(cf_matrix,xticklabels=xlabels,yticklabels=ylabels,annot=True,fmt='g',cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the percentages in each column, this will help us understand the classification report\n",
    "xlabels = ['Predicted: No','Predicted: Yes']\n",
    "ylabels = ['Actual: No', 'Actual: Yes']\n",
    "sns.heatmap(cf_matrix/np.sum(cf_matrix),xticklabels=xlabels,yticklabels=ylabels,annot=True,fmt='.2%',cmap='Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here is the classification report built from the confusion matrix<br>\n",
    "\n",
    "Learn more about the numbers here and see question below that you need to answer:  <a href=\"https://www.analyticsvidhya.com/blog/2020/04/confusion-matrix-machine-learning/\">Explaining the Confusion Matrix</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the classification report\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5:  Using the link provided above, explain the classification report and confusion matrix charts that have been built. Is the F1 score the best measure of accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training the Random Forest model\n",
    "\n",
    "Now its time to train our Random Forest Model which will built lots of decision trees and average the results.  This is called an ensemble model and is usually a great model for data that has a target variable (supervised) and can benefit from decision tree analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions and Evaluation\n",
    "\n",
    "Let's predict off the y_test values and evaluate our model.\n",
    "\n",
    "** Predict the class of fully.paid for the X_test data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_pred = rfc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,rfc_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test,rfc_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Question 6:  Which model perfomed better?  Single Decision Tree or Random Forest? How can you tell?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The end!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
