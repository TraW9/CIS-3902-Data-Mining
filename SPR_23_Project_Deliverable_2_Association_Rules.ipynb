{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "indirect-rogers",
      "metadata": {
        "id": "indirect-rogers"
      },
      "source": [
        "### Data Mining Spring 2023 Project:\n",
        "#### Understanding and predicting Shark Presence in Near Shore Waters\n",
        "#### Group Members:<br><br>\n",
        "<p> This project has two deliverables:<br>\n",
        "Deliverable 1:  Domain Understanding, Data Exploration and Preparation, Decision Trees and Random Forests (due 4/20)<br>\n",
        "Deliverable 2:  Association Rules (due 5/2 2:30 pm end of exam period)<br>\n",
        "\n",
        "\n",
        "#### Deliverable 2\n",
        "This is Deliverable 2. You will be asked to share your results of Deliverable 1 and 2 (screenshots and file) in a discussion forum. In this notebook there will be code, as well as places for you to watch a video, read an article or answer questions. </p>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "shaped-jerusalem",
      "metadata": {
        "id": "shaped-jerusalem"
      },
      "source": [
        "#### 1. A.  Import Libraries\n",
        "<p>We are importing pandas and numpy for working with data and we are also adding another library called Orange which will handle the creation of the association rules. </p><p>You can simply run this code</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "employed-hazard",
      "metadata": {
        "id": "employed-hazard"
      },
      "outputs": [],
      "source": [
        "#some code so those pesky warnings from deprecated code won't appear\n",
        "import warnings\n",
        "def fxn():\n",
        "    warnings.warn(\"deprecated\", DeprecationWarning)\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter(\"ignore\")\n",
        "    fxn()\n",
        "#the rest of the imports\n",
        "#pandas for working with datasets\n",
        "import pandas as pd\n",
        "#numpy for working with arrays\n",
        "import numpy as np\n",
        "#import Orange for the association rules - since we've never used it we need to install it first\n",
        "import sys\n",
        "!{sys.executable} -m pip install orange3\n",
        "!{sys.executable} -m pip install orange3-associate\n",
        "import Orange\n",
        "from orangecontrib.associate.fpgrowth import *  \n",
        "from Orange.data.pandas_compat import table_from_frame"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "entitled-learning",
      "metadata": {
        "id": "entitled-learning"
      },
      "source": [
        "#### 1. B.  Read in the Dataset\n",
        "<p>Now that we have all of our libraries, we read in the necessary dataset. We will be using the same shark attach dataset as the last notebook.</p><p>This time, we will be focusing on the categorical (non-numeric) attributes.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "duplicate-closer",
      "metadata": {
        "id": "duplicate-closer"
      },
      "outputs": [],
      "source": [
        "# just as in the previous notebook, we need to read in the dataset\n",
        "# we need to specify the encoding\n",
        "# remember that the csv file is posted on the class github site\n",
        "bdf = pd.read_csv('https://raw.githubusercontent.com/catawba-data-mining/CIS-3902-Data-Mining/main/sharkdata.csv', encoding=\"ISO-8859-1\")\n",
        "#let's take a look at the attributes and file size\n",
        "bdf.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "alpha-turkey",
      "metadata": {
        "id": "alpha-turkey"
      },
      "source": [
        "#### 1. C.  Transform the Attributes\n",
        "<p>This is the same code as was used in the previous notebook. We still need to transform the data into categories.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "challenging-pressing",
      "metadata": {
        "id": "challenging-pressing"
      },
      "outputs": [],
      "source": [
        "#change object type attributes - most of the discretized features - to categorical\n",
        "#object type can be difficult to visualize and model\n",
        "bdf[\"turtleexactdiscretizeSC\"] = bdf[\"turtleexactdiscretizeSC\"].astype('category')\n",
        "bdf[\"TurtleexactdiscretizeNC\"] = bdf[\"TurtleexactdiscretizeNC\"].astype('category')\n",
        "bdf[\"TurtleAttackActivityDiscretized\"] = bdf[\"TurtleAttackActivityDiscretized\"].astype('category')\n",
        "bdf[\"Area\"] = bdf[\"Area\"].astype('category')\n",
        "bdf[\"Attack\"] = bdf[\"Attack\"].astype('category')\n",
        "bdf[\"Timeofattack\"] = bdf[\"Timeofattack\"].astype('category')\n",
        "bdf[\"Beach\"] = bdf[\"Beach\"].astype('category')\n",
        "bdf[\"DissolvedO2discretize\"] = bdf[\"DissolvedO2discretize\"].astype('category')\n",
        "bdf[\"salinitydiscretize\"] = bdf[\"salinitydiscretize\"].astype('category')\n",
        "bdf[\"turbiditydiscretize\"] = bdf[\"turbiditydiscretize\"].astype('category')\n",
        "bdf[\"temperaturediscretize\"] = bdf[\"temperaturediscretize\"].astype('category')\n",
        "bdf[\"precipitationdiscretize\"] = bdf[\"precipitationdiscretize\"].astype('category')\n",
        "bdf[\"pressurediscretize\"] = bdf[\"pressurediscretize\"].astype('category')\n",
        "bdf[\"windspeeddiscretize\"] = bdf[\"windspeeddiscretize\"].astype('category')\n",
        "bdf[\"precipitationmvadiscretize\"] = bdf[\"precipitationmvadiscretize\"].astype('category')\n",
        "bdf[\"CrabLandingsDisc\"] = bdf[\"CrabLandingsDisc\"].astype('category')\n",
        "bdf[\"Direction\"] = bdf[\"Direction\"].astype('category')\n",
        "bdf[\"DirectionDisc\"] = bdf[\"DirectionDisc\"].astype('category')\n",
        "bdf[\"DirectionDiscInt\"] = bdf[\"DirectionDiscInt\"].astype('category')\n",
        "bdf[\"MoonPhaseCat\"] = bdf[\"MoonPhase\"].astype('category')\n",
        "bdf[\"MoonPhaseCatExtend\"] = bdf[\"MoonPhaseIntExtend\"].astype('category')\n",
        "#change attack and moonphase cat to codes to help with scatter matrix visualization\n",
        "#MoonPhaseCat is the actual MoonPhase as a string\n",
        "#MoonPhaseCatExtended is the Extended MoonPhase\n",
        "#0 is Quarter moons, 1 is wan gibb and wax cres, 2 is wax gibb and wan cres, 3 is Full and New\n",
        "#DirectionDiscInt is the Wind Direction discretized\n",
        "#NE = 1, E = 2, SE = 3, S = 4, W = 5, SW = 6\n",
        "bdf[\"AttackCat\"] = bdf[\"Attack\"].cat.codes\n",
        "bdf[\"MoonPhaseCatExtendCodes\"] = bdf[\"MoonPhaseCatExtend\"].cat.codes\n",
        "bdf[\"DirectionDiscIntCodes\"] = bdf[\"DirectionDiscInt\"].cat.codes\n",
        "#fix date time\n",
        "bdf[\"Date\"] = bdf[\"Date\"].astype('category')\n",
        "format_str = '%d/%m/%Y' # The format\n",
        "bdf[\"Date\"] = bdf[\"Date\"].apply(pd.to_datetime)\n",
        "#datetime.datetime.strptime(bdf[\"Date\"], format_str)\n",
        "#print info again on data frame and attributes\n",
        "bdf.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "altered-theme",
      "metadata": {
        "id": "altered-theme"
      },
      "outputs": [],
      "source": [
        "#let's take a quick peek at the dataset to make sure everything looks good\n",
        "bdf.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "possible-approach",
      "metadata": {
        "id": "possible-approach"
      },
      "source": [
        "#### 1. D.  Select the relevant columns\n",
        "<p>Now, we select the columns that we want to use for modeling. We are using ar, for association rules, for the set of columns.</p><p>Like before we won't use two correlated attributes for modeling.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "incorrect-experiment",
      "metadata": {
        "id": "incorrect-experiment"
      },
      "outputs": [],
      "source": [
        "# now, we pick some of the attributes we are interested in for generating rules\n",
        "ar = bdf[[\"Attack\", \"DissolvedO2discretize\", \"salinitydiscretize\",\n",
        "          \"turbiditydiscretize\", \"pressurediscretize\", \"windspeeddiscretize\", \n",
        "          \"DirectionDisc\", \"CrabLandingsDisc\", \"MoonPhaseCatExtend\"]]\n",
        "#take a look\n",
        "ar.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mobile-taste",
      "metadata": {
        "id": "mobile-taste"
      },
      "outputs": [],
      "source": [
        "# and let's look at it as a dataframe\n",
        "ar.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "likely-final",
      "metadata": {
        "id": "likely-final"
      },
      "source": [
        "<font color=RED>QUESTION: </font> <p>We are only using discrete features for this portion. Could any of the other attributes be transformed to be used?  How would you do that?</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "faced-flash",
      "metadata": {
        "id": "faced-flash"
      },
      "source": [
        "#### 2. A.  Encode the table"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yellow-library",
      "metadata": {
        "id": "yellow-library"
      },
      "source": [
        "<p>We are looking for groups of attributes that occur frequently together. However, we need to do a bit of preproccessing first. </p>\n",
        "\n",
        "<p>We need to use something called OneHot encoding to prepare the dataset. OneHot transforms the data so that every possible value is it's own column. Then, it labels the columns as 0 or 1 depending on if that value is true or false. The number of columns grows very fast, but the data is very simple. \n",
        "    \n",
        "<p>Read this article on <a href=\"https://medium.com/@michaeldelsole/what-is-one-hot-encoding-and-how-to-do-it-f0ae272f1179\">OneHot Encoding</a>.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "exclusive-richardson",
      "metadata": {
        "id": "exclusive-richardson"
      },
      "outputs": [],
      "source": [
        "# In this code, X is the new table being created and mapping stores the list of changes so we can understand our output. \n",
        "X, mapping = OneHot.encode(table_from_frame(ar))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "measured-parish",
      "metadata": {
        "id": "measured-parish"
      },
      "outputs": [],
      "source": [
        "# This is the transformed table.\n",
        "X"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rubber-tunnel",
      "metadata": {
        "id": "rubber-tunnel"
      },
      "source": [
        "#### 2. B.  Determine Frequent Itemsets"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "productive-ladder",
      "metadata": {
        "id": "productive-ladder"
      },
      "source": [
        "Frequent itemsets are groups of values appear together often. When discussing frequent itemsets, we often use the term support. Support is a measure of how often the item appears. We aren't usually interested in a group that only shows up one time. Usually, we use either a specific number or a percentage of our dataset. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "satellite-afternoon",
      "metadata": {
        "id": "satellite-afternoon"
      },
      "source": [
        "<a href=\"https://youtu.be/TcUlzuQ27iQ\">Video on the algorithm being used to make the item sets. It goes on to form representative rules which are a special variation of association rules.</a><br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "requested-identifier",
      "metadata": {
        "id": "requested-identifier"
      },
      "outputs": [],
      "source": [
        "# Now, we can call frequent itemsets from the Orange library.\n",
        "# We require that the itemsets must have a support of 0.1. \n",
        "# To be considered an itemset must appear in at least 10% of our data\n",
        "frequent_sets = dict(frequent_itemsets(X, .1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "quick-illness",
      "metadata": {
        "id": "quick-illness"
      },
      "outputs": [],
      "source": [
        "# Here is what the itemsets look like. They are still encoded. \n",
        "frequent_sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "other-latin",
      "metadata": {
        "id": "other-latin"
      },
      "outputs": [],
      "source": [
        "# Each key corresponds to a specific value. The code below decodes the table and shows what the it means. \n",
        "key_names = {item: '{}={}'.format(var.name, val)\n",
        "  for item, var, val in OneHot.decode(mapping, table_from_frame(ar), mapping)}\n",
        "key_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "maritime-civilian",
      "metadata": {
        "id": "maritime-civilian"
      },
      "outputs": [],
      "source": [
        "# We store the key values for the attack category for later. We are interested in 1, which represents Attack=Yes. \n",
        "attack_keys = {1}\n",
        "attack_keys"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sunset-daughter",
      "metadata": {
        "id": "sunset-daughter"
      },
      "source": [
        "If you compare the list of key_names to the itemsets we can see which items appear together most often. Some of the sets only contain one item. \n",
        "\n",
        "However, this is awkward to read, and would become time consuming to work with, so we will decode the sets and transform them back into words. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "universal-speech",
      "metadata": {
        "id": "universal-speech"
      },
      "outputs": [],
      "source": [
        "for set in frequent_sets:\n",
        "    print(', '.join(key_names[i] for i in set), \"- support: \", frequent_sets[set])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "unauthorized-rwanda",
      "metadata": {
        "id": "unauthorized-rwanda"
      },
      "source": [
        "#### 3. A.  Generate Association Rules"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "secret-washer",
      "metadata": {
        "id": "secret-washer"
      },
      "source": [
        "So, now we have some frequent itemsets. We can use these to make association rules. These are one step further, and state that when some items are together, there is usually another specific item as well. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "loaded-quick",
      "metadata": {
        "id": "loaded-quick"
      },
      "source": [
        "When we generate the rules, we specify a minimum confidence. Confidence is a measure of how often the rule proves to be true. It is calculated by dividing the number of times the rule is true, over the total number of times the conditions for the rule appears. In this case, we will set the minimum confidence to 50%."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "czech-professor",
      "metadata": {
        "id": "czech-professor"
      },
      "source": [
        "These rules have two parts, an antecedent and a consequent. The antecedent is the components and the consquent is the result. So, if you have rule that says high salinity implies no attack then the salinity is the antecdent and the attack is the consequent. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mechanical-equality",
      "metadata": {
        "id": "mechanical-equality"
      },
      "source": [
        "The last part restricts the rules to ones that have a consequent and ones where that consequent is Attack=Yes. One of the downsides of association rules is that many uninteresting rules are generated. So, we usually restrict the rules we examine in some way. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lovely-george",
      "metadata": {
        "id": "lovely-george"
      },
      "outputs": [],
      "source": [
        "# We can use the library again to generate the rules. \n",
        "# We generate them using the package and then place them in a list. \n",
        "# P and Q represent the two parts of the rule, the antecedent and the consequent\n",
        "rules = [(P, Q, supp, conf)\n",
        " for P, Q, supp, conf in association_rules(frequent_sets, .5)\n",
        " if len(Q) == 1 and Q & attack_keys]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "related-veteran",
      "metadata": {
        "id": "related-veteran"
      },
      "outputs": [],
      "source": [
        "# We print the first 20 rules\n",
        "rules[:20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "appreciated-following",
      "metadata": {
        "scrolled": false,
        "id": "appreciated-following"
      },
      "outputs": [],
      "source": [
        "# Finally, we use key_names to transform the rules into something easily readable and print the first 20. \n",
        "for ante, cons, supp, conf in rules[:20]:\n",
        "     print(', '.join(key_names[i] for i in ante), '-->',\n",
        "           key_names[next(iter(cons))],\n",
        "           '(supp: {}, conf: {})'.format(supp, conf))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "valued-science",
      "metadata": {
        "id": "valued-science"
      },
      "source": [
        "<font color=RED>QUESTION: </font> <p>After generating all of this do you notice any interesting patterns in the rules? List some ways you could modify the above code to learn more.</p>"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.12"
    },
    "colab": {
      "name": "Project_Deliverable_2_Association_Rules.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
